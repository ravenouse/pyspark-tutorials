{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTWjz2HJ4MaH"
      },
      "source": [
        "# PySpark Tutorial\n",
        "<div>\n",
        " <h2> CSCI 4283 / 5253 \n",
        "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\"/> </h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv26vOeB4MaJ"
      },
      "source": [
        "Spark was originally developed using Scala, although there are Python and Java interfaces as well. This tutorial covers [most of the RDD API](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds) using Python bindings.\n",
        "\n",
        "You may want to consult the [PySpark manual](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html) as well."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "nL0cnVCd5JNl",
        "outputId": "f44600d0-617d-423c-c461-6f178a9c1e69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 50 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 51.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=0e6b0471fc456569a62f71c584bd25213fc4b3ea8e56783e7fb902d53a67ce25\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Vgac6zqY4MaK"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "import numpy as np\n",
        "import operator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK5z3hOH4MaL"
      },
      "source": [
        "Note that I am using an explicit declaration of the number of local processes to use with `local[3]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AJC2TgQM4MaL"
      },
      "outputs": [],
      "source": [
        "conf=SparkConf().setAppName(\"pyspark tutorial\").setMaster(\"local[3]\")\n",
        "sc = SparkContext(conf=conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49pPRY4Y4MaL"
      },
      "source": [
        "## Partitions\n",
        "\n",
        "RDD's are broken into multiple partitions or slices which are the unit of work allocation (*i.e.*, more partitions gives more potential for parallelism, but too many partitions gives too much overhead). By default, the number of partitions is related to your cluster size. In this example, I uses `local[3]` to specify three worker processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lUWEjHOo4MaM"
      },
      "outputs": [],
      "source": [
        "a = sc.parallelize([7, 2, 3, 1, 2, 3, 4, 5, 6, 7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RqKxFQmz4MaM",
        "outputId": "b3bbb781-2612-47fa-da38-f1ee7f434a8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "a.getNumPartitions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09AolHh14MaM"
      },
      "source": [
        "We can also specify the number of partitions or slices when parallelizing a data structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T_rCgqsZ4MaN"
      },
      "outputs": [],
      "source": [
        "a2 = sc.parallelize([7, 2, 3, 1, 2, 3, 4, 5, 6, 7], numSlices=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VUJJz6Um4MaN",
        "outputId": "c3fb7595-8d43-40dc-f276-8a4ca928bc63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "a2.getNumPartitions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0z6wtQI4MaN"
      },
      "source": [
        "## When are partitions visible?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfQ4VnnL4MaN"
      },
      "source": [
        "In general, you don't need to be aware of partitions / slices but some code uses the underlying partition information when implementing other operations. We'll look at one example, **fold**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eJoAqbE4MaO"
      },
      "source": [
        "**fold** takes a \"identity value\" and a function and then repeatedly performs a reduction on the RDD using the identity value and function and then again when the values have be collected at the host. `fold` is a general version of `reduce` that handles the case of data across multiple partitions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DCn_6xf4MaO"
      },
      "source": [
        "For example, assume we want to create a list from elements of an RDD using list addition. There are numerous reasons why this is a bad idea, but it helps us illustrate the impact of partitions.\n",
        "\n",
        "The problem is that you can't use \"list addition\" until you have a list. For example, you need to execute:\n",
        "` [] + [1] ` before you can create a list using `[] + [1] + [2]`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JPRzalyS4MaO",
        "outputId": "4d25ffa5-c63c-4227-c64b-fd1853885a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "[] + [1] + [2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAtwqdsW4MaO"
      },
      "source": [
        "**fold** only combines elements in a partition -- **reduce** basically does a **fold** and then combines the results from the individual partitions. Let's apply **fold** to a 3-parition structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pA3ymkFl4MaP",
        "outputId": "6a7b8fdd-b572-400a-98a1-43a3df208089",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[7, 2, 3], [1, 2, 3], [4, 5, 6, 7]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "a.fold([], lambda x,y: x + [y])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QClDIHNo4MaP"
      },
      "source": [
        "Underneath the hood of **reduce** there's a set of tools that handle operations within a partition and then across partitions. We're going to look at the general **aggregate** method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAoUQxq54MaP"
      },
      "source": [
        "### Aggregate -- generalization of reduce and fold\n",
        "\n",
        "In order to understand the order of operations, we need a function that will illustrate that order for an RDD. The **showAdd** function will show the order of operations using parenthesis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "c02Dl0784MaP"
      },
      "outputs": [],
      "source": [
        "def showAdd(x,y):\n",
        "    return f\"({x} + {y})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0VkWWPqr4MaQ",
        "outputId": "cffde447-c15e-4cf3-b655-87b7c7538ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'((1 + 2) + 3)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "showAdd( showAdd(1,2), 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ggdeNjpO4MaQ",
        "outputId": "120e0dca-9070-4cc7-8dd3-b3d21fda87a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'((((2 + 3) + 4) + 5) + 6)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "oneslice = sc.parallelize([2,3,4,5,6],1)\n",
        "oneslice.reduce(showAdd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nhyc0swU4MaQ"
      },
      "source": [
        "If we partition the same data into two slices, we see that one partition contains `(2+3)` and the other contains `(4+5)+6`. The reduce combines these two together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xNkTgURu4MaR",
        "outputId": "d3d573ef-7c6e-4775-a6c7-ed91b193007e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'((2 + 3) + ((4 + 5) + 6))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "twoslice = sc.parallelize([2,3,4,5,6],2)\n",
        "twoslice.reduce(showAdd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijaDCpiw4MaR"
      },
      "source": [
        "Now, lets see the semantics of **fold**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "v4otgiBK4MaR",
        "outputId": "1c8855b6-8ea3-499c-e9ae-f2dee942c710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(1 + (((((1 + 2) + 3) + 4) + 5) + 6))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "oneslice.fold(1,showAdd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jksjtloP4MaR",
        "outputId": "9e2e7823-3e61-4154-8cb5-9fd1c29a610d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'((1 + ((1 + 2) + 3)) + (((1 + 4) + 5) + 6))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "twoslice.fold(1,showAdd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiFMZpfP4MaR"
      },
      "source": [
        "In other words, the identity element is added to the first element of each partition Like **reduce**, the **fold** operation really only works well for commutative-associative operators because it's applied to each slice of an RDD independently.\n",
        "\n",
        "Recall that we explicitly specified that `twoslice`should have two slices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ry7ZqDK84MaR",
        "outputId": "14bc6009-e608-4403-9ce4-42b1df8d814e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'((2 + 3) + ((4 + 5) + 6))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "twoslice.reduce(showAdd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "L7s6rRDO4MaS",
        "outputId": "8a048ceb-5501-42a2-9a4f-4ff618481489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'((1 + ((1 + 2) + 3)) + (((1 + 4) + 5) + 6))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "twoslice.fold(1, showAdd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODcplUs_4MaS"
      },
      "source": [
        "The **aggregate* function performs an operation like`fold` on each RDD partition and then uses a __combine function__ to join partitions.\n",
        "\n",
        "For example, assume we have data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0Mo670dk4MaS"
      },
      "outputs": [],
      "source": [
        "twoPart = sc.parallelize([1,2,3,4], numSlices=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DftVncFF4MaS"
      },
      "source": [
        "This data will (likely) be divided into `[1,2]` and `[3,4]`. Now assume we want to reduce two values -- the first is the sum of the data (10) and the second is the length of largest partition (likely 2).\n",
        "\n",
        "We'll have two distinct functions -- `seqOp` will define operations within a partition and `combOp` will define op how partitions are combined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uaJxEk4n4MaS"
      },
      "outputs": [],
      "source": [
        "def seqOp( x, y):\n",
        "    return f\"({x} + S + {y})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xgP68SGN4MaT"
      },
      "outputs": [],
      "source": [
        "def combOp( x, y ):\n",
        "    return f\"({x} + C + {y})\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TOZwtqQ4MaT"
      },
      "source": [
        "As with `fold`, we need a \"zero-value\" to start folding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HavLvzCG4MaT",
        "outputId": "e2e15878-9129-4ad2-e8ee-a9e4ae27a5e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(0 + C + (((((0 + S + 2) + S + 3) + S + 4) + S + 5) + S + 6))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "oneslice.aggregate( 0, seqOp, combOp )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFevsVbc4MaT"
      },
      "source": [
        "Recall that `oneslice` has a single partition. The `seqOp` operation is applied to the elements in the single RDD and combined with the identity value (0). That RDD is then combined using  `combOp` the identity value (0) and the result from the single RDD.\n",
        "\n",
        "Now, lets see what this is like for two slices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CPxg7AW14MaT",
        "outputId": "39ec044d-66c6-435a-c774-ecad40129b65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'((0 + C + ((0 + S + 2) + S + 3)) + C + (((0 + S + 4) + S + 5) + S + 6))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "twoslice.aggregate( 0, seqOp, combOp )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z954wFNF4MaT"
      },
      "source": [
        "In this case, the two values in each of the two RDD's are combined using `seqOp` and the identity element.\n",
        "\n",
        "The result from the two sequences are then combined using `compOp` in a *left to right* oerdering.\n",
        "\n",
        "**aggregate** is a the basis of many of the other operations in Spark. You can use it to build additional extensions, but many of the common operations we need are built in using **aggregate**."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}